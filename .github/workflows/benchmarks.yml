name: Benchmarks

on:
  workflow_dispatch:  # Allow manual triggering
  pull_request:
    paths:
      - 'src/**'
      - 'benchmark/**'
      - '.github/workflows/benchmarks.yml'
  push:
    branches:
      - main
    paths:
      - 'src/**'
      - 'benchmark/**'

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 90

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Julia
        uses: julia-actions/setup-julia@v2
        with:
          version: '1'
          arch: x64

      - name: Cache Julia packages
        uses: actions/cache@v4
        with:
          path: |
            ~/.julia/artifacts
            ~/.julia/packages
            ~/.julia/compiled
          key: ${{ runner.os }}-julia-benchmark-${{ hashFiles('**/Project.toml', '**/Manifest.toml') }}
          restore-keys: |
            ${{ runner.os }}-julia-benchmark-
            ${{ runner.os }}-julia-

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libfftw3-dev

      - name: Setup benchmark environments
        run: |
          # Setup FFTA environment
          echo "Setting up FFTA environment..."
          cd benchmark/ffta_env
          julia --project=. -e '
            import Pkg
            Pkg.develop(path="../..")
            Pkg.instantiate()
            Pkg.precompile()
          '
          cd ../..

          # Setup FFTW environment
          echo "Setting up FFTW environment..."
          cd benchmark/fftw_env
          julia --project=. -e '
            import Pkg
            Pkg.instantiate()
            Pkg.precompile()
          '
          cd ../..

          # Setup plotting environment
          echo "Setting up plotting environment..."
          cd benchmark
          julia --project=. -e '
            import Pkg
            Pkg.instantiate()
            Pkg.precompile()
          '
          cd ..

      - name: Run benchmarks
        id: run_benchmarks
        run: |
          cd benchmark
          set -o pipefail
          julia --project=. run_benchmarks.jl 2>&1 | tee benchmark_log.txt

      - name: Check benchmark results
        id: check_results
        if: success()
        run: |
          if [ -f "benchmark/results_ffta.json" ] && [ -f "benchmark/results_fftw.json" ]; then
            echo "results_exist=true" >> $GITHUB_OUTPUT
            echo "âœ… Benchmark results generated successfully"
          else
            echo "results_exist=false" >> $GITHUB_OUTPUT
            echo "âŒ Benchmark results not found"
            exit 1
          fi

          if [ -f "benchmark/benchmark_report.html" ]; then
            echo "html_exists=true" >> $GITHUB_OUTPUT
            echo "âœ… HTML report generated successfully"
          else
            echo "html_exists=false" >> $GITHUB_OUTPUT
            echo "âš ï¸ HTML report not found"
            exit 1
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results-${{ github.run_id }}
          path: |
            benchmark/benchmark_report.html
            benchmark/*.json
            benchmark/benchmark_log.txt
          retention-days: 30
          if-no-files-found: warn

      - name: Comment benchmark results on PR
        if: github.event_name == 'pull_request' && success()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            try {
              // Read benchmark results
              const fftaResults = JSON.parse(fs.readFileSync('benchmark/results_ffta.json', 'utf8'));
              const fftwResults = JSON.parse(fs.readFileSync('benchmark/results_fftw.json', 'utf8'));

              // Create summary table
              let comment = '## ðŸ“Š Benchmark Results\n\n';
              comment += 'Performance comparison between FFTA.jl and FFTW.jl:\n\n';
              comment += '| Category | Array Size | FFTA (Î¼s) | FFTW (Î¼s) | Speedup |\n';
              comment += '|----------|------------|-----------|-----------|----------|\n';

              const categories = {
                'odd_power_of_2': 'Odd Powers of 2',
                'even_power_of_2': 'Even Powers of 2',
                'power_of_3': 'Powers of 3',
                'composite': 'Composite'
              };

              // Sample sizes from each category
              const sampleSizes = {
                'odd_power_of_2': [8, 128, 2048, 32768],
                'even_power_of_2': [16, 256, 4096],
                'power_of_3': [9, 243, 6561],
                'composite': [840]
              };

              for (const [catKey, catName] of Object.entries(categories)) {
                const sizes = sampleSizes[catKey] || [];
                for (const size of sizes) {
                  const fftaData = fftaResults.data.find(d => d.size === size && d.category === catKey);
                  const fftwData = fftwResults.data.find(d => d.size === size && d.category === catKey);

                  if (fftaData && fftwData) {
                    const fftaTime = (fftaData.median_time * 1e6).toFixed(3);
                    const fftwTime = (fftwData.median_time * 1e6).toFixed(3);
                    const speedup = fftwData.median_time / fftaData.median_time;
                    const speedupText = speedup > 1
                      ? `${speedup.toFixed(2)}x (FFTA faster)`
                      : `${(1/speedup).toFixed(2)}x (FFTW faster)`;

                    comment += `| ${catName} | ${size} | ${fftaTime} | ${fftwTime} | ${speedupText} |\n`;
                  }
                }
              }

              comment += '\nðŸ“ˆ **[Download detailed HTML report and plots from artifacts]';
              comment += '(${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})**\n';
              comment += '\n---\n';
              comment += '*Benchmarks run with BenchmarkTools.jl (100 samples, 10 evaluations per sample)*';

              // Post comment
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });

              console.log('âœ… Posted benchmark results to PR');
            } catch (error) {
              console.error('âŒ Failed to post PR comment:', error);
              // Don't fail the workflow if commenting fails
            }

      - name: Generate job summary
        if: success()
        run: |
          echo "## ðŸ“Š Benchmark Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… **Benchmarks completed successfully!**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Count total tests
          FFTA_COUNT=$(grep -o '"size"' benchmark/results_ffta.json | wc -l)
          echo "- Tested $FFTA_COUNT array sizes across 4 categories" >> $GITHUB_STEP_SUMMARY
          echo "- Categories: Odd powers of 2, Even powers of 2, Powers of 3, Composite" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo "ðŸ“ˆ **Outputs generated:**" >> $GITHUB_STEP_SUMMARY
          echo "- Interactive HTML report with embedded plots" >> $GITHUB_STEP_SUMMARY
          echo "- Category-specific performance plots" >> $GITHUB_STEP_SUMMARY
          echo "- Combined performance comparison plots" >> $GITHUB_STEP_SUMMARY
          echo "- Raw JSON results for further analysis" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“¦ **Artifacts:** Download benchmark results from the artifacts section above" >> $GITHUB_STEP_SUMMARY
